=Benthis WP2 workflow example=

Welcome to the Benthis workflow example wiki. The page is hosted on the VMStools page
as for this analyses you will be required to install the VMStools software for R.
Please note that at this stage (May 2013), VMStools will only operate on R-2.15.x versions.

==Objectives==
  * To map habitat types and sea bed impact from fishing activities in EU waters to identify where fisheries potentially compromises seafloor integrity and conflicts of ecosystem services
  * To develop and implement new methodology, combining VMS, logbook and industry data, to assess actual seabed impact from large scale fishing activities on an appropriate spatial and temporal scale

==workflow==

Please find below an example workflow to treat VMS and Logbook in a systematic manner to create swept area maps.


=== Installing VMStools ===
  * The VMStools software is available [http://code.google.com/p/vmstools/downloads/list here]
  * The procedure for installing the VMStools library under R is described [http://code.google.com/p/vmstools/wiki/Introduction?tm=6 here]
  
=== Loading the required libraries into the R console ===
We load three different libraries, to start with the VMStools one, followed by two packages that help us to create maps. If you have not installed them, please use {{{install.packages("maps",repos=getOption("repos")}}} and select the correct packages.
<code>  
rm(list=ls())
library(vmstools)
library(maps)
library(mapdata)
memory.size(4000)
</code>  
  
In general, workflows can be easily adapted to each users' computer by setting working directories first. You may need to adjust these to direct to the <br> folders you would like to store the results in.

=== setting working directories ===

<code>  
codePath  <- "D:/Benthis/Code/R/"
dataPath  <- "D:/Benthis/Data/"
pricePath <- "D:/Benthis/Data/Price/"
outPath   <- "D:/Benthis/Output/"
polPath   <- "D:/Benthis/Data/Polygons/"
</code>

=== loading VMS and Logbook data ===
 
The VMS data we use in this analyses must be formatted to the 'tacsat' style. The logbook data must be formatted in the 'eflalo' style. You can download a description of these formats [http://code.google.com/p/vmstools/downloads/list here] and if you want another example, in the VMStools package an example tacsat and eflalo dataset are embedded. Load the VMStools library and call these by typing {{{data(tacsat)}}} or {{{data(eflalo)}}}.
If you have the data in R format already, you can read them in as below.
Make sure that they are in the right column format too!
<code>  
load(file.path(dataPath,"tacsat.RData")); # get the tacsat object
load(file.path(dataPath,"eflalo.RData")); # get the eflalo object
tacsat <- formatTacsat(tacsat) # format each of the columns to the specified class
eflalo <- formatEflalo(eflalo) # format each of the columns to the specified class
</code>  

If they are in a .csv format, you could use the following lines.

<code>
tacsat <- readTacsat(file.path(dataPath,"tacsat.csv"))
eflalo <- readEflalo(file.path(dataPath,"eflalo.csv"))
</code>

Make sure that the fishing activities / metier (the 'LE_MET' column in eflalo) is present and complete.
 
=== load VMStools additional datasets ===
<code>  
data(euharbours)
data(ICESareas)
data(europa)
</code>  

Let's take a look at e.g. the ICES areas and harbours dataset.

<code>  
map("worldHires",xlim=c(-15,15),ylim=c(40,65),fill=T,col="#7FBC41",oma=rep(0,4),mar=rep(2.5,4))
plot(ICESareas,add=T,col="lightblue")
points(harbours$lon,harbours$lat,pch=16,col="blue",cex=0.4)
map.axes()
</code>  
It should look something like this:<br>
[http://vmstools.googlecode.com/svn/wiki/figures/harbours.png]


=== Cleaning the tacsat dataset ===
In general, the tacsat dataset contains many errors, such as duplicates, points that are on land or not even on the globe. These need to be filtered out before we execute the analyses. Most of these 'problems' have been identified by different authors of peer-reviewed papers, and we treat the most common ones below.

  
  * It is a good idea to keep track of those records you have removed 
<code>
remrecsTacsat     <- matrix(NA,nrow=6,ncol=2,dimnames= list(c("total","duplicates","notPossible",
                                                              "pseudoDuplicates","harbour","land"),
                                                            c("rows","percentage")))
remrecsTacsat["total",] <- c(nrow(tacsat),"100%")
</code>

  * Remove duplicate records 
<code>
tacsat$SI_DATIM <- as.POSIXct(paste(tacsat$SI_DATE,  tacsat$SI_TIME,   sep=" "),
                              tz="GMT", format="%d/%m/%Y  %H:%M")
uniqueTacsat    <- paste(tacsat$VE_REF,tacsat$SI_LATI,tacsat$SI_LONG,tacsat$SI_DATIM)
tacsat          <- tacsat[!duplicated(uniqueTacsat),]
remrecsTacsat["duplicates",] <- c(nrow(tacsat),100+round((nrow(tacsat) -
                an(remrecsTacsat["total",1]))/an(remrecsTacsat["total",1])*100,2))
</code>

  * Remove points that cannot be possible
<code>
spThres         <- 20   #Maximum speed threshold in analyses in nm
  idx           <- which(abs(tacsat$SI_LATI) > 90 | abs(tacsat$SI_LONG) > 180)
  idx           <- unique(c(idx,which(tacsat$SI_HE < 0 | tacsat$SI_HE > 360)))
  idx           <- unique(c(idx,which(tacsat$SI_SP > spThres)))
if(length(idx)>0) tacsat          <- tacsat[-idx,]
remrecsTacsat["notPossible",] <- c(nrow(tacsat),100+round((nrow(tacsat) -
                an(remrecsTacsat["total",1]))/an(remrecsTacsat["total",1])*100,2))
</code>

  * Remove points which are pseudo duplicates as they have an interval rate < x minutes
<code>
intThres        <- 5    # Minimum difference in time interval in minutes to prevent pseudo duplicates
tacsat          <- sortTacsat(tacsat)
tacsatp         <- intervalTacsat(tacsat,level="vessel",fill.na=T)
tacsat          <- tacsatp[which(tacsatp$INTV > intThres | is.na(tacsatp$INTV)==T),-grep("INTV",colnames(tacsatp))]
remrecsTacsat["pseudoDuplicates",] <- c(nrow(tacsat),100+round((nrow(tacsat) -
                an(remrecsTacsat["total",1]))/an(remrecsTacsat["total",1])*100,2))
</code>

  * Remove points in harbour
<code>
idx             <- pointInHarbour(tacsat$SI_LONG,tacsat$SI_LATI,harbours)
pih             <- tacsat[which(idx == 1),]
save(pih,file=paste(outPath,"pointInHarbour.RData",sep=""))
tacsat          <- tacsat[which(idx == 0),]
remrecsTacsat["harbour",] <- c(nrow(tacsat),100+round((nrow(tacsat) -
                an(remrecsTacsat["total",1]))/an(remrecsTacsat["total",1])*100,2))
</code>

  * Remove points on land
<code>
pols            <- lonLat2SpatialPolygons(lst=lapply(as.list(sort(unique(europa$SID))),
                        function(x){data.frame(SI_LONG=subset(europa,SID==x)$X,SI_LATI=subset(europa,SID==x)$Y)}))
idx             <- pointOnLand(tacsat,pols);
pol             <- tacsat[which(idx == 1),]
save(pol,file=paste(outPath,"pointOnLand.RData",sep=""))
tacsat          <- tacsat[which(idx == 0),]
remrecsTacsat["land",] <- c(nrow(tacsat),100+round((nrow(tacsat) -
                an(remrecsTacsat["total",1]))/an(remrecsTacsat["total",1])*100,2))
</code>
If you would plot those points on land, it would look something like this:<br>
[http://vmstools.googlecode.com/svn/wiki/figures/pointsOnLand.png]


  * Save the remrecsTacsat file
<code>
save(remrecsTacsat,file=file.path(outPath,"remrecsTacsat.RData"))
</code>
The result, based on the VMStools tacsat dataset, looks like:
|| *Category* || *Records left* || *Percentage of original* ||
|| total || 97015 || 100% ||
|| duplicates || 97015 || 100% ||
|| not possible || 97001 || 99.99% ||
|| pseudo duplicates || 84774 || 87.38% ||
|| in harbour || 70948 || 73.13% ||
|| on land || 70063 || 72.22% ||

  * Save the cleaned tacsat file
<code>
save(tacsat,file=file.path(outPath,"cleanTacsat.RData"))
</code>  

 
=== cleaning eflalo ===
Very similar to cleaning the tacsat dataset, we need to clean the eflalo dataset too.

  * Keep track of removed points
<code>
remrecsEflalo     <- matrix(NA,nrow=5,ncol=2,dimnames=list(c("total","duplicated","impossible time",
                                                             "before 1st Jan","departArrival"),
                                                           c("rows","percentage")))
remrecsEflalo["total",] <- c(nrow(eflalo),"100%")
</code>

  * Remove non-unique trip numbers
<code>
eflalo            <- eflalo[!duplicated(paste(eflalo$LE_ID,eflalo$LE_CDAT,sep="-")),]
remrecsEflalo["duplicated",] <- c(nrow(eflalo),100+round((nrow(eflalo) -
                  an(remrecsEflalo["total",1]))/an(remrecsEflalo["total",1])*100,2))
</code>

  * Remove impossible time stamp records
<code>
eflalo$FT_DDATIM  <- as.POSIXct(paste(eflalo$FT_DDAT,eflalo$FT_DTIME, sep = " "),
                               tz = "GMT", format = "%d/%m/%Y  %H:%M")
eflalo$FT_LDATIM  <- as.POSIXct(paste(eflalo$FT_LDAT,eflalo$FT_LTIME, sep = " "),
                               tz = "GMT", format = "%d/%m/%Y  %H:%M") 

eflalo            <- eflalo[!(is.na(eflalo$FT_DDATIM) |is.na(eflalo$FT_LDATIM)),]
remrecsEflalo["impossible time",] <- c(nrow(eflalo),100+round((nrow(eflalo) -
                  an(remrecsEflalo["total",1]))/an(remrecsEflalo["total",1])*100,2))
</code>
  
  * Remove trip starting befor 1st Jan
<code>
year              <- min(year(eflalo$FT_DDATIM))
eflalo            <- eflalo[eflalo$FT_DDATIM>=strptime(paste(year,"-01-01 00:00:00",sep=''),
                                                             "%Y-%m-%d %H:%M:%S"),]
remrecsEflalo["before 1st Jan",] <- c(nrow(eflalo),100+round((nrow(eflalo) -
                  an(remrecsEflalo["total",1]))/an(remrecsEflalo["total",1])*100,2))
</code>

  * Remove records with arrival date before departure date
<code>
eflalop           <- eflalo
eflalop$FT_DDATIM <- as.POSIXct(paste(eflalo$FT_DDAT,  eflalo$FT_DTIME,   sep=" "),
                                tz="GMT", format="%d/%m/%Y  %H:%M")
eflalop$FT_LDATIM <- as.POSIXct(paste(eflalo$FT_LDAT,  eflalo$FT_LTIME,   sep=" "),
                                tz="GMT", format="%d/%m/%Y  %H:%M")
idx               <- which(eflalop$FT_LDATIM >= eflalop$FT_DDATIM)
eflalo            <- eflalo[idx,]
remrecsEflalo["departArrival",] <- c(nrow(eflalo),100+round((nrow(eflalo) -
                  an(remrecsEflalo["total",1]))/an(remrecsEflalo["total",1])*100,2))
</code>

  * Save the remrecsEflalo file
<code>
save(remrecsEflalo,file=file.path(outPath,"remrecsEflalo.RData"))
</code>

The result, based on the VMStools eflalo dataset, looks like:
|| *Category* || *Records left* || *Percentage of original* ||
|| total || 4539 || 100% ||
|| duplicated || 4446 || 97.95% ||
|| impossible time || 4446 || 97.95% ||
|| before 1st of Jan || 4446 || 97.95% ||
|| departure before arrival || 4446 || 97.95% ||

  * Save the cleaned eflalo file
<code>
save(eflalo,file=file.path(outPath,"cleanEflalo.RData"))
</code>  


As we now have cleaned both the tacsat and eflalo datasets, we can start to explore
the potential of combining these datasets. Both datasets have two aspects in common:
vessel name or identifier and a date / time stamp. On this basis, both datasets
can be merged together. One tries to find those tacsat records that fall within
the trip departure and arrival date registered in the logbooks.

In VMStools, a special function is designed to do this automatically.

=== merge eflalo and tacsat ===

The code itself is simple. Note that the trip identifier (FT_REF) in the eflalo
dataset is assigned to the tacsat file if a match is found. If not, the FT_REF
column in the tacsat dataset will state '0'.
<code>  
tacsatp           <- mergeEflalo2Tacsat(eflalo,tacsat)
</code>  

As we've now established some links, we can assign more attributes from the
eflalo data to the tacsat data, such as gear characteristics (LE_GEAR), vessel
length (VE_LEN) or fishery type (LE_MET).

  * Link gear characteristics to tacsat

<code>
tacsatp$LE_GEAR   <- eflalo$LE_GEAR[match(tacsatp$FT_REF,eflalo$FT_REF)] 
tacsatp$VE_LEN    <- eflalo$VE_LEN[ match(tacsatp$FT_REF,eflalo$FT_REF)] 
tacsatp$LE_MET    <- eflalo$LE_MET[ match(tacsatp$FT_REF,eflalo$FT_REF)] 
save(tacsatp,   file=paste(outPath,"tacsatMerged.RData",   sep=""))
</code>
In some cases, no match could be found. The result will look something like this:<br>
[http://vmstools.googlecode.com/svn/wiki/figures/missingMatch.png]

  * read in look-up table (industry data from interviews / experimental studies)
To calculate bottom impact in a later stage, we need the width of the gear. It is not
always easy to derive these widths though.
The gear width for a (beam) trawl is straightforward, the gear width for a seine
could be the length of the net and the gear width of a gillnets or other could be
the number of nets released per operation times the unit net length. However
, this needs to be confirmed by the industry and potentially making use of a functional
relationships between vessel HP or length to gear size.

A table specifying all these gear - width combinations should be put together and
read into the workflow.

Here we create a fake gearwidth table as an example and merge it to the tacsat
dataset.
<code>  
vesselGear            <- tacsatp[!duplicated(data.frame(tacsatp$VE_REF,tacsatp$LE_GEAR)), ]
fakeGearWidth         <- vesselGear[,c('VE_REF', 'LE_GEAR')]
fakeGearWidth         <- fakeGearWidth[-which(is.na(fakeGearWidth$VE_REF) | is.na(fakeGearWidth$LE_GEAR)),]
fakeGearWidth$LE_GEAR_WIDTH <- 0.5 # in km
save(fakeGearWidth, file=paste(dataPath,"gearWidth.RData",   sep=""))
  
load(file.path(dataPath, "gearWidth.RData"))
tacsatp               <- merge(tacsatp, fakeGearWidth,by=c("VE_REF","LE_GEAR"),
                               all.x=T,all.y=F)
    
save(tacsatp,   file=paste(outPath,"tacsatMergedWidth.RData",   sep=""))
</code>  


=== Separate tacsat that is not merged ===
<code>  
tacsatpmin        <- subset(tacsatp,FT_REF == 0)
save(tacsatpmin, file=paste(outPath,"tacsatNotMerged.RData",sep=""))
</code>  

=== define (fishing) activity ===
A common way to define activity of a vessel is to look at its speed profiles.
In general, you can identify for most bottom trawling gear 3 peaks. One peak
would be situated right at the zero-knots boundary, one peak somewhere between
4-8 knots and a third one between 8 and 12 knots. For example, see the speed profile
below (top: instantaneous speeds, below: calculated speeds):<br>
[http://vmstools.googlecode.com/svn/wiki/figures/speedProfiles.png]


The following step identifies the fishing activities from the steaming phases
by analysing the vessel-specific speed profile and knowing the type of gear used from the logbooks.


First remove points with NA's in them in critial places
<code>
idx               <- which(is.na(tacsatp$VE_REF) == T   | is.na(tacsatp$SI_LONG) == T | is.na(tacsatp$SI_LATI) == T |
                           is.na(tacsatp$SI_DATIM) == T |  is.na(tacsatp$SI_SP) == T)
if(length(idx)>0) tacsatp         <- tacsatp[-idx,]
</code>

The identification of these activities is sensitive to starting conditions on where
the peaks are approximately. Hence, a pre-analyses needs to take place to roughly
estimate where each of the three peaks is situated. You can either spot 2 or 3 peaks
(one around zero knots, around 4-8 knots and around 8-12 knots or one around zero
nots and one around 6-12 knots). To reliably estimate these peaks, the speed profile
is mirrored in the y-axis and therefore you can either spot 3 or 5 peaks. The
following routine will ask for the number of peaks and where they roughly are
situated.
<code>
storeScheme       <- activityTacsatAnalyse(tacsatp, units = "year", analyse.by = "LE_GEAR",identify="means")
storeScheme       <- storeScheme[-which(is.na(storeScheme$analyse.by)==T),]
save(storeScheme, file=paste(outPath,"storeScheme.RData",sep=""))
</code>
In case you have many vessels, you could create the storeScheme object yourself outside
of this function.

It is in many cases easier to perform the calculation on a year-by-year basis.
<code>
storeScheme       <- storeScheme[storeScheme$years==1800,]
storeScheme$years <- as.numeric(as.character(storeScheme$years))
tacsatp$year      <- format(tacsatp$SI_DATIM, "%Y")
tacsatp           <- tacsatp[tacsatp$year %in% 1800,]
</code>

The following routine is the function that actually identifies the peaks and the
result is returned in the 'activity' object.
<code>
activity          <- activityTacsat(tacsatp,units="year",analyse.by="LE_GEAR",storeScheme,
                                plot=TRUE,level="all",sigma=1)
tacsatp$SI_STATE  <- activity
</code>

In most cases however, you have still a few records that have no activity associated
as yet. You need to 'fix' this manually. An example is given below.

<code>
idx               <- which(is.na(tacsatp$SI_STATE))
tacsatp$SI_STATE[idx[which(tacsatp$SI_SP[idx] >= 1.5 &
                           tacsatp$SI_SP[idx] <= 7.5)]] <- 'f'
tacsatp$SI_STATE[idx[which(tacsatp$SI_SP[idx] <  1.5)]] <- 'h'
tacsatp$SI_STATE[idx[which(tacsatp$SI_SP[idx] >  7.5)]] <- 's'
</code>

Make sure you save the results, else you might need to do the analyse all over!
<code>
save(storeScheme, file=file.path(outPath,"storeScheme.RData"))
save(tacsatp,     file=file.path(outPath,"tacsatActivity.RData"))
</code>  

=== Alternative activity identification: (Based on Fock 2008) ===

We can read in a table that specifies speed ranges for vessels fishing for different
gears. This table can either be constructed on the basis of published work by
Fock 2008 (Fisheries in the context of marine spatial planning: Defining principal
areas for fisheries in the German EEZ) or on a dataset created yourself.

<code>
gearSpeeds            <- read.csv(file.path(dataPath,"gearSpeeds.csv"))
colnames(gearSpeeds)  <- c("LE_GEAR","SI_SPmin","SI_SPmax")
#- In this example, gearSpeeds is a table with 3 columns. One column = gear code,
#   second column is the minimum speed associatd with fishing and the third
#   column is the maximum speed associated with fishing

tacsatp$SI_STATE <- 's'
for(iGear in sort(gearSpeeds$LE_GEAR)){
  idxGearSpeed  <- subset(gearSpeeds,LE_GEAR == iGear)
  idxtacsat     <- which(tacsatp$LE_GEAR == iGear &
                         tacsatp$SI_SP >= gearSpeeds$SI_SPmin &
                         tascatp$SI_SP <= gearSpeeds$SI_SPmax)
  tacsatp$SI_STATE[idx] <- 'f'
}
</code>


=== Labelling each haul ===
Assign an identifier (HL_ID) to each of the fishing sequences. This can especially
be useful when you want to count the number of hauls in an aggregated grid.

<code>
tacsatp   <- labellingHauls(tacsatp)
</code>

=== Interpolate tacsat data (or not) ===
In some occasions, and especially for towed trawl gear, it can be useful to artificially
increase the density of VMS points. Through the use of an interpolation algorithm
this is possible. Based on speed and heading a spline will be fitted between two
consecutive fishing pings and the result can be converted into the tacsat format
again, now containing an additional set of intermediate pings.

<code>
load(file=paste(outPath,"tacsatActivity.RData",   sep=""))

#- Select towed gears only
towed_gears       <- c('OTB', 'TBB', 'PTB', 'PTM', 'DRB')  # TO DO: list to be checked
tacsatp           <- orderBy(~VE_REF+SI_DATIM,data=tacsatp)
</code>

As these interpolations can become rather large, it is sensible to loop over the
gears and store the results in between. Therefore, we first create an output folder
by typing {{{dir.create(file.path(outPath, "interpolated"))}}}.

Interpolation should only occur on the fishing points, so we need to get rid to
all non-fishing points. To be conservative, we start each 'haul' with a steaming
point and finish with a steaming point too.

<code>
tacsatp$SI_STATE_num <- NA
tacsatp$SI_STATE_num[which(tacsatp$SI_STATE=="h")] <- 1
tacsatp$SI_STATE_num[tacsatp$SI_STATE=="f"] <- 2
tacsatp$SI_STATE_num[tacsatp$SI_STATE=="s"] <- 3
is_transition     <- c(0,diff(tacsatp$SI_STATE_num))
is_transition2    <- c(diff(tacsatp$SI_STATE_num), 0)
tacsatp           <- tacsatp[ !is.na(tacsatp$SI_STATE_num) & (tacsatp$SI_STATE_num ==2 |
                                      is_transition!=0 | is_transition2!=0),]
tacsatp           <- tacsatp[,-grep("SI_STATE_num",colnames(tacsatp))]
tacsatp$SI_STATE  <- "f"
</code>

Now that we remain with fishing points only, we can perform the interpolation

<code>
for(iGr in towed_gears){
  tacsatpGear        <- tacsatp[!is.na(tacsatp$LE_GEAR) & tacsatp$LE_GEAR==iGr,]

  for(iVE_REF in sort(unique(tacsatpGear$VE_REF))){
    tacsatpGearVEREF <- tacsatpGear[tacsatpGear$VE_REF %in% iVE_REF,]
    if(nrow(tacsatpGearVEREF)>3) {

    #- Interpolate according to the cubic-hermite spline interpolation
    interpolationcHs <- interpolateTacsat(tacsatpGearVEREF,
                          interval=120, # note that the ping rate is likely to be specific to the country
                          margin=10, # i.e. will make disconnected interpolations if interval out of the 110 130 min range
                          res=100,
                          method="cHs",
                          params=list(fm=0.2,distscale=20,sigline=0.2,st=c(2,6)),
                          headingAdjustment=0,
                          fast=FALSE)

    #- Convert the interpolation to tacsat style data
    tacsatIntGearVEREF <- interpolation2Tacsat(interpolationcHs, tacsatpGearVEREF)

    #- Correct for HL_ID skipping (not in use if labellingHauls() called before)
    #tacsatIntGearVEREF$HL_ID[which(diff(tacsatIntGearVEREF$HL_ID)<0)] <- tacsatIntGearVEREF$HL_ID[(which(diff(tacsatIntGearVEREF$HL_ID)<0)-1)]
    
    #- Add swept area (see explanation below)
    #tacsatIntGearVEREF$SWEPT_AREA_KM2 <- NA
    #tacsatIntGearVEREF$SWEPT_AREA_KM2 <- distance(c(tacsatIntGearVEREF$SI_LONG[-1],0),  c(tacsatIntGearVEREF$SI_LATI[-1],0),
    #                                            tacsatIntGearVEREF$SI_LONG,           tacsatIntGearVEREF$SI_LATI) *
    #                                          tacsatIntGearVEREF$LE_GEAR_WIDTH

    save(tacsatIntGearVEREF, file=file.path(outPath, "interpolated",
          paste("tacsatInt",iVE_REF, "_", iGr, ".RData", sep="")))
    }
  }
}
</code>

This is how an interpolation could look like, including the sampled points from it.
The blue points are the original VMS pings while the black solid line represents
the connection of all interpolated points (100). The 8 intermediate red points are
sampled from this set of 100 (representing equal distance between points).<br>
[http://vmstools.googlecode.com/svn/wiki/figures/interpolation.png]

Additionally, we can calculate the swept area as we know its distacnce travelled
and the width of the gear.

<code>
tacsatIntGearVEREF$SWEPT_AREA_KM2 <- NA
tacsatIntGearVEREF$SWEPT_AREA_KM2 <- distance(c(tacsatIntGearVEREF$SI_LONG[-1],0),  c(tacsatIntGearVEREF$SI_LATI[-1],0),
                                                tacsatIntGearVEREF$SI_LONG,           tacsatIntGearVEREF$SI_LATI) *
                                              tacsatIntGearVEREF$LE_GEAR_WIDTH
</code>

=== Buffer area ===

For those vessels where we cannot interpolate (because it doesn't make sense,
we can apply a certain buffer area, representing some sort of circle around a
VMS ping. To derive the swept area from that circle we apply simple mathematics.

We do that here for those gear types that were not in our towed_gear list.

<code>
all_gears            <- sort(unique(tacsatp$LE_GEAR))
passive_gears        <- all_gears[!all_gears %in% towed_gears]

for(iGr in passive_gears){
  tacsatpGear        <- tacsatp[!is.na(tacsatp$LE_GEAR) & tacsatp$LE_GEAR==iGr,]

  for(iVE_REF in sort(unique(tacsatpGear$VE_REF)){
    tacsatpGearVEREF <- tacsatpGear[tacsatpGear$VE_REF %in% iVE_REF,]
    tacsatpGearVEREF <- tacsatpGearVEREF[tacsatpGearVEREF$SI_STATE=='f',] # keep fishing pings only
  
    tacsatpGearVEREF$SWEPT_AREA_KM2 <- pi*(tacsatpGearVEREF$LE_GEAR_WIDTH/(2*pi))^2

    tacsatIntGearVEREF <- tacsatpGearVEREF
   
    save(tacsatIntGearVEREF, file=file.path(outPath, "interpolated",
       paste("tacsatSweptArea_",iVE_REF, "_", iGr, ".RData", sep="")))
  }
} 
</code>

=== Create dataset incl. swept area and effort ===

As we've now calculated the swept area for both the interpolated gear types and
the remaining ones via the buffer method, we can now combine all of them.

<code>
allInts <- list()
counter <- 0
for(iGr in all_gears){
  for(iVE_REF in sort(unique(tacsatpVE_REF))){

    dat <- try(get(load(file=file.path(outPath, "interpolated",
                    paste("tacsatSweptArea_",iVE_REF, "_", iGr, ".RData", sep="")))))
    #- Assign effort to dataset
    tacsatp$effort_KWdays    <- intervalTacsat(tacsatp,level="trip",fill.na=T)$INTV * as.numeric(as.character(tacsatp$VE_KW))

    if(class(dat) != "try-error"){
      counter <- counter + 1
      allInts[[counter]] <- dat
    }
  }
}
tacsatp <- do.call(rbind,allInts)
save(tacsatp,file=file.path(outPath,"tacsatSweptArea.RData"))
</code>

=== Link habitat map to vms ===

For some regions, habitat maps are available as shapefiles. If these datasets
exist, we can read them into R and overlay the VMS points with the habitat map.
Through this overlap we can identify a habitat type with VMS ping.

First we load the habitat map and load the tacsat data with swept area indication.
Thereafter, we convert the tacsat data into a 'coordinate' dataset which we then
can overlay with the habitat map. The result of that exercise is a vertor which
indicates which habitat can be associated with each VMS ping. Thereafter we assign
this habitat type to the tacsat file.

* Via shapefile

<code>
  library(maptools)

  # load a habitat map shape file (Baltic)
  habitat_map           <- readShapePoly(file.path(polPath,"sediment_lat_long"),
                                         proj4string=CRS("+proj=longlat +ellps=WGS84"))
  # load a habitat map shape file (North Sea)
  habitat_map           <- readShapePoly(file.path(polPath,"ModelledSeabedHabitats"),
                                         proj4string=CRS("+proj=longlat +ellps=WGS84"))

  # get 'tacsatp' with all data
  load(file.path(outPath, "tacsatSweptArea.RData"))

  # Turn the habitat map into spatial polygons
  sp <- SpatialPolygons(habitat_map@polygons)
  proj4string(sp) <-  CRS("+proj=longlat +ellps=WGS84")

  #Turn the VMS point into a spatial points object
  spo             <- SpatialPoints(coordinates(data.frame(SI_LONG=tacsatp$SI_LONG,
                                                          SI_LATI=tacsatp$SI_LATI)))
  proj4string(spo) <-  CRS("+proj=longlat +ellps=WGS84")

  # Use the magic 'over' function to match habitat type with VMS position
  idx <- over(spo,sp)
  
  #- Baltic
  tacsatp$SUBSTRATE <- habitat_map$BAL_CODE[idx]

  #- North Sea
  tacsatp$SUBSTRATE <- habitat_map$substrate[idx]

  # Make a plot of the result (baltic)
  plot(habitat_map, xlim=c(11,14), ylim=c(55,56))
  axis(1) ; axis(2, las=2) ; box()
  points(tacsatp[, c("SI_LONG","SI_LATI")], col=tacsatp$SUBSTRATE, pch=".")
  
  # Make a plot of the results (North Sea)
  plot(1,1,col="white",xlim=c(-4,10), ylim=c(50,60),xlab="Longitude",ylab="Latitude")
  plot(habitat_map, add=T,border="grey")
  box()
  points(tacsatp[, c("SI_LONG","SI_LATI")], col=tacsatp$SUBSTRATE, pch=".")

  # Save the plot
  savePlot(filename=file.path(outPath, "VMSpingsAttachedToSedimentMap.jpeg"), type="jpeg")
</code>

The result might look something like this:<br>

[http://vmstools.googlecode.com/svn/wiki/figures/habitatMapNorthSea.png]

Here we've used shapefiles of habitat type and linked VMS points, but the habitat
maps might be available in raster types only (already gridded files such as a TIF)

* Via raster

<code>
  
  sh_coastlines            <- readShapePoly(file.path(polPath,"francois_EU"))
 
  ## use point-raster overlay.......
  library(raster)
  landscapes       <- raster(file.path(polPath, "landscapes.tif"))    # probably need an update of rgdal here....
  newproj          <- "+proj=longlat +datum=WGS84"
  landscapes_proj  <- projectRaster(landscapes, crs=newproj)
  
  save(landscapes_proj, file=file.path(polPath, "landscapes_proj.RData"))
  
  load(file.path(polPath, "landscapes_proj.RData")) 
 
  load(file.path(outPath, "tacsatSweptArea.RData")) # get 'tacsatp' with all data 
  #....or load only one instance eg load("C:\\merging\\BENTHIS\\outputs\\interpolated\\tacsatSweptArea_DNK000005269_OTB.RData"))
  #tacsatp <- tacsatInt_gr_vid

  
  coord <- cbind(x=anf(tacsatp$SI_LONG), y=anf(tacsatp$SI_LATI))
  
  dd <- extract (landscapes_proj, coord[,1:2]) # get the landscape on the coord points!

  coord <- cbind(coord,  landscapes_code=cut(dd, breaks=c(0,100,200,300,400,500,600)))

  tacsatp <- cbind(tacsatp, landscapes_code= coord[,'landscapes_code'])
  
  # plot and save...
  plot(landscapes_proj, xlim=c(10,14), ylim=c(54.5,56.5))
  plot(sh_coastlines,  xlim=c(10,14), ylim=c(54.5,56.5), add=TRUE)  # better for plotting the western baltic sea coastline!
  points(coord[,"x"], coord[,"y"], col=coord[,"landscapes_code"], pch=".", cex=1)

  # save
  savePlot(filename=file.path(outPath, "VMSpingsAttachedToLandscapeMap.jpeg"), type="jpeg")
  
</code>  




  * severity of activity: read in a look-up table (from interviews / experimental studies)


<code>  
  ## by gear/metier by habitat
  ## (i.e. a mutiplying factor in relative terms)
     
  # create a fake input file to show the required format
  dd <- tacsatp[!duplicated(data.frame(tacsatp$VE_REF,tacsatp$LE_GEAR, tacsatp$LE_MET)), ] 
  fake_gear_metier_habitat_severity_table <- dd[,c('VE_REF', 'LE_GEAR', 'LE_MET')]
  fake_gear_metier_habitat_severity_table <- cbind(fake_gear_metier_habitat_severity_table, HAB_SEVERITY=1)
  gear_metier_habitat_severity_table  <- fake_gear_metier_habitat_severity_table  
  gear_metier_habitat_severity_table  <-   gear_metier_habitat_severity_table[complete.cases( gear_metier_habitat_severity_table),] 
  save(gear_metier_habitat_severity_table,   file=paste(dataPath,"gear_metier_habitat_severity_table.RData",   sep=""))
  
  # load a table for HAB_SEVERITY per vid LE_REF per gr LE_GEAR per met LE_MET
  load(file.path(dataPath, "gear_metier_habitat_severity_table.RData"))
  tacsatp <- merge(tacsatp, gear_metier_habitat_severity_table) 
  save(tacsatp,   file=paste(outPath,"tacsatMergedHabSeverity.RData",   sep=""))
</code>  


  * pressure: weigh the swept area by the severity

<code>  
tacsatp$pressure <- tacsatp$HAB_SEVERITY * tacsatp$SWEPT_AREA_KM2
</code>  



  * define grid
    * mid-point polygon distance distribution (to help defining a suitable grid resolution)
<code>  
</code>  


 





    * random process of fishing activity (to help defining a suitable grid resolution)

<code>  
</code>  






  * maps + sum / average different grids
  using a quick gridding code at various resolution.

    * For example, grid the swept area, or the number of hauls, or the fishing pressure, etc.
<code>  
   ##-----------------------------------------------------------------------------
   # 
  
  ## GRIDDING (IN DECIMAL DEGREES OR IN UTM COORD)
  # using a quick gridding code at various resolution.

  # For example, grid the swept area, or the number of hauls, or the fishing pressure, etc.
  sh1 <- readShapePoly(file.path(polPath,"francois_EU")) # coastline



  ### the swept area-------------------------------------------

   ## user selection here----
    what                 <- "SWEPT_AREA_KM2"
    #what                <- "HL_ID"
    #what                <- "effort_days"
    #what                <- "effort_KWdays"
    #what                <- "pressure"
    #a_func              <- function(x) {unique(length(x))} # for HL_ID, to be tested.
    a_func               <- "sum"
    is_utm               <- FALSE
    all_gears            <- sort(unique(tacsatp$LE_GEAR))
    towed_gears          <- c('OTB', 'TBB', 'PTB', 'PTM', 'DRB')  # TO DO: list to be checked
    passive_gears        <- all_gears[!all_gears %in% towed_gears]
    we <- 10; ea <- 13; no <- 59; so <- 55;
    ##------------------------

    # subset for relevant fisheries
    this            <- tacsatp [tacsatp$LE_GEAR %in% towed_gears, ]

    # restrict the study area 
    # (it is likely that the same bounding box should be used when stacking different layers e.g. from different countries)
    this <- this[this$SI_LONG>we & this$SI_LONG<ea & this$SI_LATI>so & this$SI_LATI<no,]

    # grid the data (in decimal or in UTM)
    if(is_utm){
      dx <- 0.0002 # 5km x 5km
      # convert to UTM
      library(sp)
      library(rgdal)
      SP <- SpatialPoints(cbind(as.numeric(as.character(this$SI_LONG)), as.numeric(as.character(this$SI_LATI))),
                       proj4string=CRS("+proj=longlat +datum=WGS84"))
      this <- cbind(this,
                 spTransform(SP, CRS("+proj=utm  +ellps=intl +zone=32 +towgs84=-84,-107,-120,0,0,0,0,0")))    # convert to UTM
      this            <- this [, c('SI_LONG', 'SI_LATI', 'SI_DATE', 'coords.x1', 'coords.x2', what)]
      this$round_long <- round(as.numeric(as.character(this$coords.x1))*dx)
      this$round_lat  <- round(as.numeric(as.character(this$coords.x2))*dx)
      this            <- this[, !colnames(this) %in% c('coords.x1', 'coords.x2')]
      this$cell       <- paste("C_",this$round_long,"_", this$round_lat, sep='')
      this$xs         <- (this$round_long/(dx))
      this$ys         <- (this$round_lat/(dx))
 
    }  else {
      dx <- 20 # 0.05 degree
      this <- this [, c('SI_LONG', 'SI_LATI', 'SI_DATE', what)]
      this$round_long <- round(as.numeric(as.character(this$SI_LONG))*dx*2) # 0.1
      this$round_lat  <- round(as.numeric(as.character(this$SI_LATI))*dx)   # 0.05
      this$cell       <- paste("C_",this$round_long,"_", this$round_lat, sep='')
      this$xs         <- (this$round_long/(dx*2))
      this$ys         <- (this$round_lat/(dx))
    }
     # if the coordinates in decimal then dx=20 corresponds to grid resolution of 0.05 degrees
     # i.e. a 3´ angle = 3nm in latitude but vary in longitude (note that a finer grid will be produced if a higher value for dx is put here)

    colnames(this) <- c('x', 'y', 'date', 'what', 'round_long', 'round_lat', 'cell', 'xs', 'ys')


    # retrieve the geo resolution in degree, for info
    #long <- seq(1,15,by=0.01)
    #res_long <- diff( long [1+which(diff(round(long*dx)/dx/2)!=0)] )
    #res_lat <- diff( long [1+which(diff(round(long*dx/2)/dx)!=0)] )
    #print(res_long) ; print(res_lat)


    # a quick gridding method...
    background <- expand.grid(
                              x=0,
                              y=0,
                              date=0,
                              what=0,
                              round_long=seq(range(this$round_long)[1], range(this$round_long)[2], by=1),
                              round_lat=seq(range(this$round_lat)[1], range(this$round_lat)[2], by=1),
                              cell=0,
                              xs=0,
                              ys=0
                              )
    this <- rbind(this, background)
    the_points <- tapply(this$what,
                  list(this$round_lat, this$round_long), a_func)

    xs <- (as.numeric(as.character(colnames(the_points)))/(dx*2))
    ys <- (as.numeric(as.character(rownames(the_points)))/(dx))

    the_breaks <-  c(0, (1:12)^3.5 ) # to be decided...
    graphics:::image(
     x=xs,
     y=ys,
     z= t(the_points)  ,
     breaks=c(the_breaks),
     col = terrain.colors(length(the_breaks)-1),
     useRaster=FALSE,
     xlab="",
     ylab="",
     axes=FALSE,
     xlim=range(xs), ylim=range(ys),
     add=FALSE
     )
    title("")

    # land
    sh1 <- readShapePoly(file.path(polPath,"francois_EU"),  proj4string=CRS("+proj=longlat +datum=WGS84"))
    if(is_utm) sh1 <- spTransform(sh1, CRS("+proj=utm  +ellps=intl +zone=32 +towgs84=-84,-107,-120,0,0,0,0,0"))
    plot(sh1, add=TRUE, col=grey(0.7))

    legend("topright", fill=terrain.colors(length(the_breaks)-1),
             legend=round(the_breaks[-1],1), bty="n", cex=0.8, ncol=2, title="")
    box()
    axis(1)
    axis(2, las=2)

    if(is_utm){
      mtext(side=1, "Eastings", cex=1, adj=0.5, line=2)
      mtext(side=2, "Northings", cex=1, adj=0.5, line=2)
    } else{
      mtext(side=1, "Longitude", cex=1, adj=0.5, line=2)
      mtext(side=2, "Latitude", cex=1, adj=0.5, line=2)
      #points (tacsatp [,c('SI_LONG', 'SI_LATI')], pch=".", col="white")
    }
    
    
    # save
    savePlot(filename=file.path(outPath, "GriddedSweepAreaExample.jpeg"), type="jpeg")


    # export the quantity per cell and date
    library(data.table)
    DT                <-  data.table(this)
    qu                <-  quote(list(sum(what)))
    quantity_per_cell <- DT[,eval(qu), by=list(cell, xs,ys)]
    quantity_per_date <- DT[,eval(qu), by=list(date, xs,ys)]
    quantity_per_cell_date <- DT[,eval(qu), by=list(cell,date, xs,ys)]
    quantity_per_cell_date <- as.data.frame(quantity_per_cell_date)
    colnames(quantity_per_cell_date)   <- c('cell','date', 'xs','ys', 'quantity')
    rm(DT) ; gc(reset=TRUE)
    save(quantity_per_cell_date, res_long, res_lat,  we, ea, no, so, file=file.path(outPath,"quantity_per_cell_date.RData") )


    # export the cumul per cell and date
    quantity_per_cell_date <- orderBy(~date, data=quantity_per_cell_date)
    quantity_cumul_per_cell_date <- do.call("rbind", lapply(
      split(quantity_per_cell_date, f=quantity_per_cell_date$cell),
               function(x){
               x$quantity <- cumsum(x$quantity)
               x
               })  )
    # check the cumul on a given cell
    # head(quantity_cumul_per_cell_date[quantity_cumul_per_cell_date$cell=="C_197_2722",])
    save(quantity_cumul_per_cell_date, res_long, res_lat,  we, ea, no, so, file=file.path(outPath,"quantity_cumul_per_cell_date.RData") )



           

</code>  
 
 
 [[Image:http://code.google.com/p/vmstools/source/browse/wiki/GriddedSweepAreaExample.jpeg|thumb|center|upright=2.0|alt=Gridded Swept Area. 
 Gridded Swept Area example in km2 for one vessel in the Baltic Sea. | Original VMS pings are overlaid]] 


or using createGrid()
<code>  
</code>  

...and stack some layers making use of mosaic() from raster layers.

<code>  
# from ascgridfile eg. returned by VMSGridCreate()
# TO DO:
dd <- 
nn <- 
fun1  <-  function(x)  { x[ is.na(x)]  <- 0;  return(x)  }            # remove NA such zero
dd <-  calc(dd,  fun1)
nn <-  calc(nn,  fun1)
a_stack <-mosaic(dd, nn, na.rm=TRUE, fun = sum, keepres=TRUE) 
</code>  


  * non-vms maps. For non VMS-equipped vessels, the mapping of the fishing effort
  can be generated at the ICES rectangle resolution as a proxy of the fishing pressure on the benthic habitats 

<code> 
# TO DO from e.g. plotTools()
#....
# TO DO: stack several layers with various geographical resolutions 
</code>  





  * testing

<code>  
</code>


  